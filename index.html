<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DTTDNet: Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <p style="font-size: 20px;">
                <span style="color: #00A400;">Zixun Huang*</span>, 
                <span style="color: #FF4D6D;">Keling Yao*</span>, 
                <span style="color: #4169E1;">Seth Z. Zhao</span>, <br>
                <span style="color: #00A400;">Chuanyu Pan</span>, 
                <span style="color: #00A400;">Allen Y. Yang<sup>&dagger;</sup></span>
              </p>
              <p>
                <span style="color: #00A400;">&#9654;</span> UC Berkeley 
                <span style="color: #FF4D6D;">&#9654;</span> CMU
                <span style="color: #4169E1;">&#9654;</span> UCLA
              </p>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <em>CVPR Workshop MAI</em>, 2025<br>
              </span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution; <sup>&dagger;</sup> Indicates Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/html/Huang_Robust_6DoF_Pose_Estimation_Against_Depth_Noise_and_a_Comprehensive_CVPRW_2025_paper.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->


              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/augcog/DTTD2" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://arxiv.org/abs/2309.13570" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
                <br>
                          <span class="link-block">
                <a href="https://huggingface.co/datasets/ZixunH/DTTD2-IPhone" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>DTTD-Mobile Dataset</span>
                  </a>
                </span>

                <span class="link-block">
                <a href="https://huggingface.co/datasets/ZixunH/DTTD3_Impedance" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Robotics Dataset Extension</span>
                  </a>
                </span>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/TEST.mov"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robust 6DoF pose estimation with mobile devices
            is the foundation for applications in robotics, augmented
            reality, and digital twin localization. In
            this paper, we extensively investigate the robustness
            of existing RGBD-based 6DoF pose estimation
            methods against varying levels of depth sensor
            noise. We highlight that existing 6DoF pose
            estimation methods suffer significant performance
            discrepancies due to depth measurement inaccuracies.
            In response to the robustness issue, we
            present a simple and effective transformer-based
            6DoF pose estimation approach called DTTDNet,
            featuring a novel geometric feature filtering
            module and a Chamfer distance loss for training.
            Moreover, we advance the field of robust
            6DoF pose estimation and introduce a new dataset
            â€“ Digital Twin Tracking Dataset Mobile (DTTDMobile),
            tailored for digital twin object tracking
            with noisy depth data from the mobile RGBD sensor
            suite of the Apple iPhone 14 Pro. Extensive
            experiments demonstrate that DTTDNet significantly
            outperforms state-of-the-art methods at
            least 4.32, up to 60.74 points in ADD metrics
            on the DTTD-Mobile. More importantly, our approach
            exhibits superior robustness to varying
            levels of measurement noise, setting a new benchmark
            for the robustness to noise measurements.
          </p>
        </div>
        <img src="static/images/head.png" alt="MY ALT TEXT"/>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">DTTD-Mobile Dataset</h2>
    </p>We introduce DTTD-Mobile as a novel digital-twin
    pose estimation dataset captured with mobile devices.
    We provide in-depth LiDAR depth analysis and evaluation
    metrics to illustrate the unique properties and
    complexities of mobile LiDAR data.
  </p>
    </div>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/dataset.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Left: Setup of our data acquisition pipeline. Right: 3D models of the 18 objects in DTTD-Mobile.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/dataset_detailed.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Sample visualizations of our dataset. First row: Annotations for 3D bounding boxes. Second row: Corresponding semantic
segmentation labels. Third row: Zoomed-in LiDAR depth visualizations.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/noise.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Visualization of an iPhone LiDAR depth scene that
          shows distortion and long-tail non-Gaussian noise (highlighted
          inside the red box). (a) Front view. (b) Left view. (c) Right view.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/table1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Features and statistics of different datasets.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">DTTDNet</h2>
    </p>We propose a new transformer-based 6DoF pose estimator
    with depth-robust designs on modality fusion
    and training strategies, called DTTDNet. We introduce two modules,
    Chamfer Distance Loss (CDL) and Geometric Feature Filtering
    (GFF), that enable the point-cloud encoder in DTTDNet
    to handle noisy and low-resolution LiDAR data robustly.
  </p>
  <img src="static/images/model.png" alt="MY ALT TEXT"/>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Experiments Results</h2>
    </div>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/result1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative evaluation of different methods. To further validate our approach, we provide visual evidence of our model's
effectiveness in challenging occlusion scenarios and varying lighting conditions, where other models' predictions fail but ours remain
reliable.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/reslut2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Comparison with diverse 6DoF pose estimation baselines on DTTD-Mobile dataset. We showcase AUC results of ADD-S
and ADD on all 18 objects, higher is better. Based on considered 4 baselines, our model significantly improves the accuracy on most
objects. Note that the left-most column indicates the per-object depth-ADD error.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/2025_CVPRW_MAI-DTTDNet.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{huang2025robust,
  title={Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset},
  author={Huang, Zixun and Yao, Keling and Zhao, Zhihao and Pan, Chuanyu and Yang, Allen},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={1848--1857},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>

            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>Website source code based on the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. If you want to reuse their source code, please credit them appropriately.

          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

